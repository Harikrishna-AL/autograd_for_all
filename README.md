# Autograd for all

This repository will contain the code for <b><i>autograd</i></b> in more than one language. Currently, I am starting with C. The aim is to get more control over different languages other than just python.

## Basics of Automatic Differentiation
Automatic differentiation is a technique for efficiently computing the derivatives of functions. Unlike numerical differentiation (which uses approximations) or symbolic differentiation (which manipulates mathematical expressions), autograd leverages the chain rule to compute exact gradients by breaking down functions into elementary operations.

In machine learning, autograd is essential for training models using gradient-based optimization methods. By constructing a **compute graph**—a data structure that represents the flow of computations—we can calculate gradients for any differentiable function, no matter how complex.

## Autograd in C
I'll be updating soon
